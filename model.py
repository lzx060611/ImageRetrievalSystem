# model.py
import os
import torch
import torch.nn as nn
import open_clip  # ä½ ç”¨çš„æ¨¡å‹åŠ è½½åº“
from PIL import Image
import pydicom  # å¤„ç†DICOMåŒ»å­¦å›¾åƒ
from tqdm import tqdm  # æ‰¹é‡æå–æ—¶æ˜¾ç¤ºè¿›åº¦æ¡
import warnings
import numpy as np
warnings.filterwarnings("ignore")  # å¿½ç•¥æ— å…³è­¦å‘Š
from NIH import NIH_dataset 
from torch.utils.data import DataLoader

#ä¸€ã€åŠ è½½æ¨¡å‹
# 1. æ¨¡å‹é…ç½®ï¼ˆæ”¹ç®—æ³•æ—¶ä¼˜å…ˆæ”¹è¿™éƒ¨åˆ†ï¼‰
MODEL_NAME = 'hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224'
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# äºŒ. æ‰¹é‡æå–é…ç½®ï¼ˆæ ¹æ®ä½ çš„æ•°æ®é›†è°ƒæ•´è·¯å¾„ï¼‰
IMG_ROOT = "./images_001/images"  # ä½ çš„å›¾ç‰‡æ ¹ç›®å½•
IMG_LIST = "images_001\\train_val_list.txt"  # ä½ çš„å›¾ç‰‡åˆ—è¡¨æ–‡ä»¶
BATCH_SIZE = 50                       # ä½ çš„æ‰¹æ¬¡å¤§å°
SAVE_DIR = "nih_features_db"          # ç‰¹å¾ä¿å­˜æ–‡ä»¶å¤¹
SAVE_PATH = os.path.join(SAVE_DIR, "nih_biomedclip_features.pt")  # ç‰¹å¾ä¿å­˜è·¯å¾„
#ä¸‰ã€åŠ è½½æ¨¡å‹
# ===================== HashAdapter ç±» =====================
class HashAdapter(nn.Module):
    """
    å“ˆå¸Œé€‚é…å™¨ï¼šå°†512ç»´ç‰¹å¾æ˜ å°„åˆ°64ç»´äºŒå€¼å“ˆå¸Œç 
    """
    def __init__(self, input_dim=512, output_dim=64):
        super().__init__()
        # æ·»åŠ çº¿æ€§å±‚ï¼Œæ— åç½®
        self.projection = nn.Linear(input_dim, output_dim, bias=False)
        # æ­£äº¤åˆå§‹åŒ–æƒé‡
        nn.init.orthogonal_(self.projection.weight)
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šè¾“å…¥512ç»´ç‰¹å¾ï¼Œè¾“å‡º64ç»´äºŒå€¼å“ˆå¸Œç 
        """
        # çº¿æ€§æŠ•å½±
        x = self.projection(x)
        # äºŒå€¼åŒ–ï¼šå¤§äº0ç½®ä¸º1ï¼Œå¦åˆ™ç½®ä¸º0
        x = (x > 0).float()
        return x

# ===================== æ¨¡å‹æ ¸å¿ƒé€»è¾‘ï¼ˆå°è£…æˆå¯å¤ç”¨å‡½æ•°ï¼‰=====================
def load_model():
    """
    å°è£…æ¨¡å‹åŠ è½½é€»è¾‘ï¼šæ”¹ç®—æ³•æ—¶åªéœ€ä¿®æ”¹è¿™ä¸ªå‡½æ•°ï¼
    è¿”å›ï¼šmodel, preprocess_valï¼ˆæ¨ç†é¢„å¤„ç†ï¼‰, tokenizer, hash_adapter
    """
    # è§£å†³å›½å†…HFåŠ è½½æ…¢çš„é—®é¢˜
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    
    # åŠ è½½æ¨¡å‹+é¢„å¤„ç†å·¥å…·ï¼ˆæ”¹ç®—æ³•æ—¶æ›¿æ¢è¿™é‡Œï¼Œæ¯”å¦‚æ¢æˆResNet/è‡ªå®šä¹‰æ¨¡å‹ï¼‰
    model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(MODEL_NAME)
    tokenizer = open_clip.get_tokenizer(MODEL_NAME)
    
    # åˆ›å»ºå“ˆå¸Œé€‚é…å™¨
    hash_adapter = HashAdapter(input_dim=512, output_dim=64)
    
    # æ¨¡å‹é…ç½®ï¼ˆè¯„ä¼°æ¨¡å¼+è®¾å¤‡åˆ†é…ï¼‰
    model.eval()
    hash_adapter.eval()
    model = model.to(DEVICE)
    hash_adapter = hash_adapter.to(DEVICE)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ | è®¾å¤‡ï¼š{DEVICE} | æ¨¡å‹ï¼š{MODEL_NAME}")
    print(f"âœ… HashAdapteråŠ è½½å®Œæˆ | è¾“å‡ºç»´åº¦ï¼š64 | åˆå§‹åŒ–ï¼šæ­£äº¤")
    return model, preprocess_val, tokenizer, hash_adapter

# å…¨å±€åŠ è½½æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼Œé¿å…é‡å¤åŠ è½½æµªè´¹å†…å­˜ï¼‰
model, preprocess_val, tokenizer, hash_adapter = load_model()


# ----------------------
# å››. å°è£…ç‰¹å¾æå–å‡½æ•°ï¼ˆä¾›åç«¯è°ƒç”¨ï¼‰
# è¾“å…¥ï¼šPIL.Imageå¯¹è±¡
# è¾“å‡ºï¼šå½’ä¸€åŒ–åçš„ç‰¹å¾å‘é‡ï¼ˆnumpyæ ¼å¼ï¼Œshape=(256,)ï¼‰
# ----------------------
def extract_image_feature(image):
    """
    åŠŸèƒ½ï¼šå•å¼ å›¾ç‰‡ç‰¹å¾æå–ï¼Œè¿”å›64ç»´äºŒå€¼å“ˆå¸Œç 
    å‚æ•°ï¼šimage - PIL.Imageå¯¹è±¡ï¼ˆRGBæ ¼å¼ï¼‰
    è¿”å›ï¼šnumpyæ•°ç»„ï¼ˆ64ç»´0/1å“ˆå¸Œç ï¼Œç±»å‹ä¸ºnp.uint8ï¼‰
    """
    try:
        # é¢„å¤„ç†
        processed_img = preprocess_val(image).unsqueeze(0).to(DEVICE)
        
        # ç‰¹å¾æå–
        with torch.no_grad():
            img_feature = model.encode_image(processed_img)
            # L2å½’ä¸€åŒ–
            img_feature = torch.nn.functional.normalize(img_feature, p=2, dim=1)
            # å“ˆå¸Œæ˜ å°„
            binary_code = hash_adapter(img_feature)
        
        # è½¬ä¸ºnumpyå¹¶å»é™¤batchç»´åº¦ï¼Œç±»å‹è½¬ä¸ºnp.uint8
        binary_code = binary_code.cpu().numpy()[0].astype(np.uint8)
        return binary_code
    
    except Exception as e:
        raise ValueError(f"å•å¼ å›¾ç‰‡ç‰¹å¾æå–å¤±è´¥ï¼š{str(e)}")
    

# ===================== æ‰¹é‡ç‰¹å¾æå–ï¼ˆå®Œå…¨å¤ç”¨ä½ çš„DataLoaderé€»è¾‘ï¼‰=====================
def custom_collate_fn(batch):
    """
    ä½ çš„è‡ªå®šä¹‰collate_fn
    è¾“å…¥ï¼šbatchæ˜¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯æ•°æ®é›†è¿”å›çš„ (image, img_path)
    è¾“å‡ºï¼šæ•´ç†åçš„ (å›¾åƒæ‰¹æ¬¡å¼ é‡, è·¯å¾„åˆ—è¡¨)
    """
    images, paths = zip(*batch)  # åˆ†ç¦»å›¾åƒå’Œè·¯å¾„ï¼ˆimagesæ˜¯PILå›¾åƒå…ƒç»„ï¼Œpathsæ˜¯è·¯å¾„å…ƒç»„ï¼‰
    # 1. ç”¨preprocess_valé€ä¸ªå¤„ç†PILå›¾åƒï¼Œè½¬ä¸ºå¼ é‡ï¼ˆå’Œä½ çš„é€»è¾‘ä¸€è‡´ï¼‰
    processed_images = [preprocess_val(img) for img in images]
    # 2. å°†å¤šä¸ªå¼ é‡å †å æˆæ‰¹æ¬¡ï¼ˆshape: [batch_size, 3, 224, 224]ï¼‰
    image_batch = torch.stack(processed_images)
    # 3. è·¯å¾„ç›´æ¥ä¿æŒåˆ—è¡¨å½¢å¼
    path_batch = list(paths)
    return image_batch, path_batch

def batch_extract_features():
    """
    æ‰¹é‡æå–ç‰¹å¾ï¼Œè¿”å›64ç»´äºŒå€¼å“ˆå¸Œç 
    æ— éœ€ä¼ å‚ï¼Œç›´æ¥ç”¨å…¨å±€é…ç½®é¡¹
    """
    # 1. åŠ è½½ä½ çš„NIHæ•°æ®é›†
    print(f"ğŸ“‚ åŠ è½½NIHæ•°æ®é›† | å›¾ç‰‡æ ¹è·¯å¾„ï¼š{IMG_ROOT} | åˆ—è¡¨æ–‡ä»¶ï¼š{IMG_LIST}")
    NiH_data = NIH_dataset(IMG_ROOT, IMG_LIST)
    
    # 2. åˆ›å»ºDataLoader
    NIH_loader = DataLoader(
        NiH_data, 
        batch_size=BATCH_SIZE, 
        collate_fn=custom_collate_fn
    )
    print(f"âœ… DataLoaderåˆ›å»ºå®Œæˆ | æ‰¹æ¬¡å¤§å°ï¼š{BATCH_SIZE} | æ€»æ‰¹æ¬¡ï¼š{len(NIH_loader)}")

    # 3. æ‰¹é‡æå–ç‰¹å¾
    all_binary_codes = []
    all_paths = []

    print("\nğŸš€ å¼€å§‹æ‰¹é‡æå–å“ˆå¸Œç‰¹å¾...")
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜
        # åŠ è¿›åº¦æ¡ï¼Œæ–¹ä¾¿çœ‹æå–è¿›åº¦
        for batch_images, batch_paths in tqdm(NIH_loader, desc="æå–è¿›åº¦"):
            # å›¾åƒç§»åˆ°è®¾å¤‡
            batch_images = batch_images.to(DEVICE)
            
            # æå–ç‰¹å¾
            batch_features = model.encode_image(batch_images)
            # L2å½’ä¸€åŒ–
            batch_features = torch.nn.functional.normalize(batch_features, p=2, dim=1)
            # å“ˆå¸Œæ˜ å°„
            batch_binary_codes = hash_adapter(batch_features)
            
            # ä¿å­˜å“ˆå¸Œç å’Œè·¯å¾„
            all_binary_codes.append(batch_binary_codes.cpu())
            all_paths.extend(batch_paths)

    # 4. åˆå¹¶æ‰€æœ‰å“ˆå¸Œç 
    all_binary_codes = torch.cat(all_binary_codes, dim=0)
    print(f"\nğŸ“Š æå–å®Œæˆ | å“ˆå¸Œç å½¢çŠ¶ï¼š{all_binary_codes.shape} | æœ‰æ•ˆå›¾ç‰‡æ•°ï¼š{len(all_paths)}")

    # 5. ä¿å­˜å“ˆå¸Œç å’Œè·¯å¾„
    os.makedirs(SAVE_DIR, exist_ok=True)  # åˆ›å»ºä¿å­˜æ–‡ä»¶å¤¹
    # æ›´æ–°ä¿å­˜è·¯å¾„å’Œé”®å
    SAVE_PATH = os.path.join(SAVE_DIR, "nih_biomedclip_hash_64bit.pt")
    features_db = {
        "binary_codes": all_binary_codes,  # äºŒå€¼å“ˆå¸Œç  (N, 64)
        "image_paths": all_paths   # å¯¹åº”å›¾åƒè·¯å¾„åˆ—è¡¨
    }
    torch.save(features_db, SAVE_PATH)

    print(f"âœ… å“ˆå¸Œç‰¹å¾æ•°æ®åº“å·²ä¿å­˜åˆ°ï¼š{SAVE_PATH}")
    print(f"å“ˆå¸Œç å½¢çŠ¶ï¼š{all_binary_codes.shape}ï¼ŒåŒ…å« {len(all_paths)} å¼ å›¾åƒ")

# ===================== ä¸€é”®è¿è¡Œæ‰¹é‡æå–ï¼ˆç›´æ¥æ‰§è¡Œmodel.pyå³å¯ï¼‰=====================
if __name__ == "__main__":
    # è¿è¡Œæ‰¹é‡æå–ï¼ˆå’Œä½ çš„é€»è¾‘å®Œå…¨ä¸€è‡´ï¼‰
    batch_extract_features()
